{
    "name": "[SentinelAI] Alert: Agent Refusal (Safety Filter)",
    "type": "llm-observability alert",
    "query": "llm-observability(\"@event_type:span @ml_app:sentinel-ai @sentinel.refusal:true\").rollup(\"count\").last(\"5m\") > 0",
    "message": "{{#is_alert}}\n# üõ°Ô∏è SECURITY: Agent Refusal Triggered\n\nThe Safety Filter (`sentinel.refusal:true`) blocked a request.\n\n**What this means:**\nThe agent detected a prompt that violated its safety guidelines (e.g., Jailbreak, Toxicity, Competitor Risk) and refused to answer.\n\n**Context:**\n- **Service:** {{service.name}}\n- **Trace ID:** {{trace.id}}\n\n**Recommended Action:**\n- Check the trace to classify the attack vector.\n- Ensure the refusal response was polite but firm.\n\n@pagerduty-sentinel-security\n{{/is_alert}}\n\n{{#is_alert_recovery}}\n# ‚úÖ RECOVERY: Refusal Spikes Subsided\n\nThe rate of safety refusals has dropped back to zero.\n{{/is_alert_recovery}}",
    "tags": [
        "service:sentinel-ai",
        "type:safety",
        "team:sentinel-security"
    ],
    "options": {
        "thresholds": {
            "critical": 0
        },
        "enable_logs_sample": false,
        "notify_audit": false,
        "on_missing_data": "default",
        "include_tags": true
    },
    "priority": null,
    "restricted_roles": null,
    "draft_status": "published"
}